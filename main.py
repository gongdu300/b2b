from __future__ import annotations
import os
import io
import re
import csv
import uuid
import hashlib
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from contextlib import asynccontextmanager
import pathlib, tempfile
import pandas as pd

from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from pydantic import BaseModel
from typing import List, Any
# --- DB Engine (MySQL) ---
from dotenv import load_dotenv
from sqlalchemy import create_engine, text

from gemini import translate_table_name as _translate_table_name

# --- [ADD] ë¡œê·¸ ë ˆë²¨ DEBUGë¡œ ì˜¬ë¦¬ê¸° (ENVë¡œë„ ì œì–´ ê°€ëŠ¥) ---
import logging

import re
from sqlalchemy.exc import SQLAlchemyError, ProgrammingError, IntegrityError, DataError

import pandas as pd
import numpy as np
from pydantic import BaseModel
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor


_ASCII_ID_RE = re.compile(r"^[A-Za-z0-9_]+$")

def _is_ascii_identifier(s: str) -> bool:
    """ì˜ë¬¸/ìˆ«ì/ì–¸ë”ìŠ¤ì½”ì–´ë§Œìœ¼ë¡œ êµ¬ì„±ë˜ë©´ True (ë¹ˆ ë¬¸ìì—´ì€ False)."""
    if not s:
        return False
    return bool(_ASCII_ID_RE.fullmatch(str(s)))


# --- LLM (Gemini) optional ---
try:
    from gemini.config import GEMINI_API_KEY, session, pick_model, api_url
    _HAS_LLM = bool(GEMINI_API_KEY)
except Exception:
    _HAS_LLM = False

def llm_advise_schema(headers, dtypes, samples, targets):
    """
    ìŠ¤í‚¤ë§ˆ/ìƒ˜í”Œ/íƒ€ê¹ƒì„ ìš”ì•½í•´ì„œ LLMì— 'ì‹œê³„ì—´ vs ì¼ë°˜ íšŒê·€' ì¶”ì²œë§Œ ìš”ì²­.
    ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜.
    """
    if not _HAS_LLM:
        return None
    payload_txt = (
        "You are a data science assistant. Decide for each target whether "
        "the dataset should be treated as TIME_SERIES or TABULAR REGRESSION. "
        "Return strict JSON with keys per target and value in {\"time_series\"|\"regression\"}.\n\n"
        f"Headers: {headers}\n"
        f"Dtypes: {dtypes}\n"
        f"Targets: {targets}\n"
        f"Sample rows (first 5): {samples}"
    )
    model = pick_model()
    try:
        r = session.post(
            api_url(model),
            params={"key": GEMINI_API_KEY},
            json={"contents":[{"parts":[{"text":payload_txt}]}]},
            timeout=12
        )
        if r.status_code != 200:
            return None
        js = r.json()
        txt = js.get("candidates",[{}])[0].get("content",{}).get("parts",[{}])[0].get("text","{}")
        # ë§¤ìš° ê´€ëŒ€í•œ JSON íŒŒì‹±
        import json, re
        txt_clean = re.sub(r"```json|```", "", txt).strip()
        return json.loads(txt_clean)
    except Exception:
        return None
    
# -------------------------------------------------
# ì¼ë°˜ íšŒê·€ìš© ë¹Œë” (ì‹œê°„ì»¬ëŸ¼ ì—†ì„ ë•Œ fallback)
# -------------------------------------------------
def _build_regression(df, target_col: str):
    """
    df : pandas DataFrame (ì»¬ëŸ¼ì— target_colì´ í¬í•¨ë¼ ìˆì–´ì•¼ í•¨)
    target_col : ì˜ˆì¸¡í•˜ë ¤ëŠ” ìˆ«ì ì»¬ëŸ¼

    ë¦¬í„´:
      model  : í•™ìŠµëœ ëª¨ë¸ ê°ì²´
      meta   : í•™ìŠµ ì •ë³´(dict)
    """
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor
    import numpy as np

    # 1) íƒ€ê¹ƒ/í”¼ì²˜ ë¶„ë¦¬
    if target_col not in df.columns:
        raise ValueError(f"target column '{target_col}' not found in dataframe")

    y = df[target_col]
    X = df.drop(columns=[target_col])

    # 2) ìˆ«ìí˜•ë§Œ ìš°ì„  ì‚¬ìš© (ë‚ ì§œ, ë¬¸ìì—´ ë“¤ì–´ì˜¤ë©´ ê¹¨ì§€ë‹ˆê¹Œ)
    num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    if not num_cols:
        # ìˆ«ì í”¼ì²˜ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì˜ˆì¸¡ ìì²´ê°€ ì˜ë¯¸ ì—†ìœ¼ë‹ˆ ì‹¤íŒ¨ ë¦¬í„´
        return None, {
            "ok": False,
            "reason": "no numeric feature columns to train on",
            "target": target_col,
        }

    X = X[num_cols].copy()
    # íƒ€ê¹ƒë„ ìˆ«ìë¡œ ë°”ê¿ˆ
    y = pd.to_numeric(y, errors="coerce")
    mask = y.notna()
    X = X.loc[mask]
    y = y.loc[mask]

    if len(X) < 10:
        return None, {
            "ok": False,
            "reason": "not enough rows for regression",
            "rows": int(len(X)),
            "target": target_col,
        }

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    model = RandomForestRegressor(
        n_estimators=200,
        random_state=42,
        n_jobs=-1,
    )
    model.fit(X_train, y_train)

    score = model.score(X_test, y_test)

    meta = {
        "ok": True,
        "target": target_col,
        "n_samples": int(len(df)),
        "n_features": len(num_cols),
        "features": num_cols,
        "r2": float(score),
    }
    return model, meta


# main.py ìƒë‹¨ ìœ í‹¸ë“¤ ê·¼ì²˜
def _resolve_target_name(df: pd.DataFrame, y: str) -> str | None:
    """
    ì‚¬ìš©ìê°€ ì„ íƒí•œ yê°€ df.columnsì— ì •í™•íˆ ì—†ìœ¼ë©´
    - yì™€ ë™ì¼í•˜ê±°ë‚˜
    - y + '__ìˆ«ì' (ì¤‘ë³µ ìœ ë‹ˆí¬í™”ëœ ì¼€ì´ìŠ¤)
    ì¤‘ ì²« ë²ˆì§¸ë¥¼ ìë™ ì„ íƒ.
    """
    if y in df.columns:
        return y
    cand = [c for c in df.columns if c == y or c.startswith(f"{y}__")]
    return cand[0] if cand else None

# === [ADD] ì»¬ëŸ¼ ì•ˆì „ í”½ì»¤ ===
def _pick_existing_col(df: pd.DataFrame, base: str) -> str | None:
    """
    df.columns ì•ˆì— baseê°€ ì—†ìœ¼ë©´
    - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë™ì¼
    - base + '__ìˆ«ì' (ì¤‘ë³µ ìœ ë‹ˆí¬í™”) 
    - ê³µë°±/ì–‘ë ê³µë°± ì°¨ì´
    ì¤‘ì—ì„œ ì²« ë§¤ì¹˜ë¥¼ ë°˜í™˜. ì—†ìœ¼ë©´ None.
    """
    cols = list(df.columns)
    # 1) ì •í™•íˆ
    if base in df.columns:
        return base

    # 2) strip / casefold ë™ì¼
    base_norm = str(base).strip().casefold()
    for c in cols:
        if str(c).strip().casefold() == base_norm:
            return c

    # 3) base__k (ì¤‘ë³µ ìœ ë‹ˆí¬í™”)
    for c in cols:
        if str(c).startswith(f"{base}__"):
            return c

    # 4) í”í•œ ë³€í˜•(ê³µë°± â†’ ì–¸ë”ìŠ¤ì½”ì–´)
    base_us = str(base).replace(" ", "_")
    for c in cols:
        if str(c) == base_us:
            return c

    return None





LOG_LEVEL = os.getenv("LOG_LEVEL", "debug").upper()  # í•„ìš”ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì ˆ
_level = getattr(logging, LOG_LEVEL, logging.DEBUG)

# ë£¨íŠ¸ ë¡œê±° ë° ì£¼ìš” ë¡œê±°ë“¤ ë ˆë²¨ ì„¤ì •
logging.basicConfig(level=_level)
for name in ("uvicorn", "uvicorn.error", "uvicorn.access", "fastapi"):
    logging.getLogger(name).setLevel(_level)

# (ì„ íƒ) SQLAlchemy ë“±ë„ ë³´ê³  ì‹¶ìœ¼ë©´:
# logging.getLogger("sqlalchemy.engine").setLevel(logging.DEBUG)

# [ADD] SQL ì—ëŸ¬ íœ´ë¨¸ë‹ˆì¦ˆ
from sqlalchemy.exc import ProgrammingError  # íŒŒì¼ ìƒë‹¨ ì„í¬íŠ¸ì— ì¶”ê°€

def _ensure_unique_names(names: list[str]) -> list[str]:
    seen = {}
    out = []
    for n in names:
        base = n
        if base not in seen:
            seen[base] = 1
            out.append(base)
            continue
        i = seen[base] + 1
        cand = f"{base}__{i}"
        while cand in seen:
            i += 1
            cand = f"{base}__{i}"
        seen[base] = i
        seen[cand] = 1
        out.append(cand)
    return out

# [ADD] ---- ìƒ˜í”Œ ê¸°ë°˜ íƒ€ì… ì¶”ë¡  & í–‰í¬ê¸° ì¡°ì ˆ ----
def _infer_mysql_types_from_sample(headers: list[str], text_data: str, sample_rows: int = 2000) -> list[str]:
    """
    headers/í…ìŠ¤íŠ¸ CSV ìƒ˜í”Œì„ ë³´ê³  MySQL ì»¬ëŸ¼ íƒ€ì… ë°°ì—´ì„ ë°˜í™˜.
    - ìˆ«ì(ì •ìˆ˜/ì†Œìˆ˜) â†’ BIGINT ë˜ëŠ” DECIMAL(38,6)
    - ë‚ ì§œ(YYYY-MM-DD í˜•íƒœ ìœ„ì£¼) â†’ DATE
    - ê·¸ ì™¸ ë¬¸ìì—´ â†’ ê¸¸ì´ì— ë”°ë¼ VARCHAR(16/32/64/128/191/255) ë˜ëŠ” TEXT
    """
    reader = csv.reader(io.StringIO(text_data))
    next(reader, None)  # header skip

    maxlens = [0]*len(headers)
    numcnt = [0]*len(headers)
    datecnt= [0]*len(headers)
    nonmiss=[0]*len(headers)
    decimal_seen=[False]*len(headers)

    # ê°„ë‹¨í•œ ë‚ ì§œ íŒ¨í„´(YYYY-MM-DD) ìš°ì„ 
    date_re = re.compile(r"^\d{4}-\d{2}-\d{2}$")
    num_re  = re.compile(r"^-?\d+(?:\.\d+)?$")

    for i, row in enumerate(reader):
        if i >= sample_rows:
            break
        row = (row + [""] * len(headers))[:len(headers)]
        for j, cell in enumerate(row):
            if cell is None:
                continue
            s = str(cell).strip()
            if s == "":
                continue
            nonmiss[j] += 1
            maxlens[j] = max(maxlens[j], len(s))
            if num_re.fullmatch(s):
                numcnt[j] += 1
                if "." in s:
                    decimal_seen[j] = True
            if date_re.fullmatch(s):
                datecnt[j] += 1

    types: list[str] = []
    for j in range(len(headers)):
        n = nonmiss[j] or 1
        num_ratio  = numcnt[j] / n
        date_ratio = datecnt[j] / n
        if date_ratio >= 0.9:
            types.append("DATE")
        elif num_ratio >= 0.9:
            types.append("DECIMAL(38,6)" if decimal_seen[j] else "BIGINT")
        else:
            L = maxlens[j]
            if L <= 16:    types.append("VARCHAR(16)")
            elif L <= 32:  types.append("VARCHAR(32)")
            elif L <= 64:  types.append("VARCHAR(64)")
            elif L <= 128: types.append("VARCHAR(128)")
            elif L <= 191: types.append("VARCHAR(191)")
            elif L <= 255: types.append("VARCHAR(255)")
            else:          types.append("TEXT")
    return types


def _approx_row_size(mysql_types: list[str]) -> int:
    """
    InnoDB ëŒ€ëµ í–‰ í¬ê¸° ì¶”ì •ì¹˜ (ì˜¤ë²„í—¤ë“œ ë‹¨ìˆœí™”).
    - VARCHAR(N): N + (N<=255 ? 1 : 2)
    - TEXTë¥˜: 20 (off-page pointerë¼ê³  ìƒê°)
    - BIGINT: 8, DECIMAL: 16, DATE: 3
    """
    size = 0
    for t in mysql_types:
        u = t.upper()
        if u.startswith("VARCHAR("):
            m = re.search(r"\((\d+)\)", u)
            n = int(m.group(1)) if m else 255
            size += n + (1 if n <= 255 else 2)
        elif u.startswith("BIGINT"):
            size += 8
        elif u.startswith("DECIMAL"):
            size += 16
        elif u == "DATE":
            size += 3
        elif "TEXT" in u:
            size += 20
        else:
            size += 8  # ê¸°íƒ€ ì—¬ìœ ì¹˜
    return size


def _shrink_types_to_fit(mysql_types: list[str], limit: int = 65000) -> list[str]:
    """
    í–‰ í¬ê¸°ê°€ limitë¥¼ ë„˜ìœ¼ë©´ ê°€ì¥ í° VARCHARë¶€í„° TEXTë¡œ ê°•ë“±í•˜ë©° ì¤„ì¸ë‹¤.
    """
    def largest_varchar_idx(types):
        idx, maxn = -1, -1
        for i, t in enumerate(types):
            m = re.match(r"(?i)varchar\((\d+)\)", t)
            if m:
                n = int(m.group(1))
                if n > maxn:
                    maxn, idx = n, i
        return idx

    types = mysql_types[:]
    while _approx_row_size(types) > limit:
        i = largest_varchar_idx(types)
        if i == -1:
            break  # ë” ì¤„ì¼ VARCHARê°€ ì—†ìŒ
        types[i] = "TEXT"
    return types

# [MOD] íƒ€ì… ë°°ì—´ ì§€ì› + ROW_FORMAT=DYNAMIC
def _build_ddl_from_headers(table_name: str, headers: list[str], col_types: Optional[list[str]] = None) -> str:
    t = _escape_mysql_identifier(table_name)
    if col_types is None:
        col_types = ["VARCHAR(255)"] * len(headers)
    cols = [f"  {_escape_mysql_identifier(h)} {col_types[i]} NULL" for i, h in enumerate(headers)]
    # DYNAMIC ë¡œìš°í¬ë§·: ê¸´ ê°€ë³€ê¸¸ì´ ë¬¸ìì—´ off-page ì €ì¥ì— ìœ ë¦¬
    return (
        f"CREATE TABLE {t} (\n" + ",\n".join(cols) +
        "\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ROW_FORMAT=DYNAMIC;"
    )

def _humanize_sql_error(e: Exception) -> str:
    msg = str(e)
    try:
        orig = getattr(e, "orig", None)
        if orig:
            msg = str(orig)
    except Exception:
        pass
    msg = re.sub(r"\(Background on this error.*", "", msg).strip()

    # í–‰ í¬ê¸° ì´ˆê³¼ ìš”ì•½ ì¶”ê°€
    if re.search(r"Row size too large", msg, re.IGNORECASE):
        return ("âŒ í–‰ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤(> 65535 bytes). "
                "ì»¬ëŸ¼ ìˆ˜ê°€ ë§ê±°ë‚˜ VARCHAR ê¸¸ì´ê°€ í½ë‹ˆë‹¤. ìë™ìœ¼ë¡œ TEXTë¡œ ì¡°ì •í•˜ê±°ë‚˜ ê¸¸ì´ë¥¼ ì¤„ì´ì„¸ìš”.")

    m = re.search(r"Incorrect column name '([^']+)'", msg)
    if m:
        bad = m.group(1)
        return f"âŒ ì˜ëª»ëœ ì»¬ëŸ¼ëª…: '{bad}' Â· ê³µë°±/ë¹ˆ ê°’/ì œì–´ë¬¸ì/ë”°ì˜´í‘œë¥¼ ì œê±°í•˜ê±°ë‚˜ í—¤ë”ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”."

    return f"DB ì˜¤ë¥˜: {msg}"






load_dotenv()  # .env ì½ê¸°

DB_USER = os.getenv("DB_USER", "root")
DB_PASS = os.getenv("DB_PASS", "")
DB_HOST = os.getenv("DB_HOST", "127.0.0.1")
DB_PORT = os.getenv("DB_PORT", "3306")
DB_NAME = os.getenv("DB_NAME", "data_platform")
DB_URL  = f"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}?charset=utf8mb4"
SERVER_URL = f"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/?charset=utf8mb4"  # DBëª… ì—†ì´ ì ‘ì†

ENGINE = None
STARTUP_OK = False

# ---------------------------
# Lifespan: DB/ìŠ¤í‚¤ë§ˆ ì¤€ë¹„
# ---------------------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    global STARTUP_OK, ENGINE
    try:
        # 1) DB ì—†ìœ¼ë©´ ìƒì„±
        tmp_engine = create_engine(SERVER_URL, pool_pre_ping=True)
        with tmp_engine.begin() as conn:
            conn.execute(text(f"""
                CREATE DATABASE IF NOT EXISTS `{DB_NAME}`
                CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
            """))
        tmp_engine.dispose()

        # 2) ë©”ì¸ ì—”ì§„
        ENGINE = create_engine(DB_URL, pool_pre_ping=True)

        # 3) ë©”íƒ€ í…Œì´ë¸”(ì˜µì…˜)
        with ENGINE.begin() as conn:
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS `uploads_meta` (
                id INT AUTO_INCREMENT PRIMARY KEY,
                table_name VARCHAR(128) NOT NULL,
                is_forecastable BOOLEAN DEFAULT FALSE
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
        """))
            conn.execute(text("""
                ALTER TABLE uploads_meta ADD UNIQUE KEY uq_table_name (table_name);
        """))

        STARTUP_OK = True
        print("âœ… startup: DB ë° uploads_meta í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ startup: ì´ˆê¸°í™” ì‹¤íŒ¨ â†’ {e}")

    yield
    print("ğŸ‘‹ shutdown")

# ---------------------------
# App & CORS
# ---------------------------
app = FastAPI(
    title="CSV Ingest (Schema-Aware, Auto-Merge) API",
    version="1.0.0",
    lifespan=lifespan,
)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

# ---------------------------
# In-memory cache
# ---------------------------
default_dir = os.path.join(pathlib.Path.home(), ".smartinv_uploads")

UPLOAD_DIR = os.getenv("UPLOAD_DIR", default_dir)
os.makedirs(UPLOAD_DIR, exist_ok=True)

class UploadMeta(BaseModel):
    file_id: str
    filename: str
    headers: List[str]
    header_signature: str
    header_hash: str
    num_columns: int
    uploaded_at: str

UPLOADS: Dict[str, UploadMeta] = {}
DDL_LOG: Dict[str, str] = {}

# ---------------------------
# Helpers
# ---------------------------
PREFERRED_ENCODINGS = [
    "utf-8-sig",
    "utf-8",
    "cp949",
    "euc-kr",
    "latin-1",  # fallback
]

def _decode_bytes(data: bytes) -> str:
    for enc in PREFERRED_ENCODINGS:
        try:
            return data.decode(enc)
        except Exception:
            continue
    return data.decode("utf-8", errors="replace")

# ---------------------------
# ì‹œê³„ì—´ ë°ì´í„° ê°ì§€ í—¬í¼
# ---------------------------
def _detect_timeseries_like(headers: list[str], text_data: str, col_types: list[str]) -> bool:
    """
    ì‹œê³„ì—´ / ìˆ˜ìš”ì˜ˆì¸¡ìš© ë°ì´í„°ì¸ì§€ íŒì • (íŒ¨ë„í˜•ë„ í†µê³¼ì‹œí‚´)
    ê·œì¹™ ìˆœì„œ:
    1) ì‹œì  ì»¬ëŸ¼(Date, DateTime, ë‚ ì§œ ê³„ì—´) ìˆëŠ”ì§€
    2) ì‹œì  ê°’ì´ ì‹¤ì œ ë‚ ì§œ/ì‹œê°„ìœ¼ë¡œ íŒŒì‹±ë˜ëŠ”ì§€
    3) ê°™ì€ ë‚ ì§œì— ì—¬ëŸ¬ í–‰ì´ ìˆì–´ë„, 'ì œí’ˆ/í’ˆëª©/ì°½ê³ ' ê°™ì€ í‚¤ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í—ˆìš©
    4) 'T+1', 'T+2', 'T+7', 'ì˜ˆì • ìˆ˜ì£¼ëŸ‰', 'ì˜ˆìƒ ìˆ˜ì£¼ëŸ‰' ê°™ì€ horizon ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ê°•í•˜ê²Œ True
    5) ìˆ«ìí˜• í”¼ì²˜ê°€ 2ê°œ ì´ìƒì¸ì§€
    """
    import csv, io, re
    import pandas as pd
    import numpy as np

    if any("datetime" in h.lower() for h in headers) and any("product" in h.lower() for h in headers):
        return True


    # 0) í—¤ë” ì „ì²˜ë¦¬
    raw_headers = headers
    lower_headers = [h.strip().lower() for h in raw_headers]

    # 1) ë‚ ì§œ/ì‹œê°„ í›„ë³´ ì°¾ê¸°
    date_keywords = ["date", "datetime", "time", "day", "ë‚ ì§œ", "ì¼ì", "ê¸°ì¤€ì¼", "ê±°ë˜ì¼", "ì˜ì—…ì¼"]
    date_cols = [h for h in raw_headers if any(k in h.strip().lower() for k in date_keywords)]
    if not date_cols:
        # ë‚ ì§œê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ì‹œê³„ì—´ë¡œ ì•ˆ ë³¸ë‹¤
        return False

    # 2) CSV -> DF
    reader = csv.reader(io.StringIO(text_data))
    rows = list(reader)
    if len(rows) < 2:
        return False
    df = pd.DataFrame(rows[1:], columns=raw_headers)
    if df.empty:
        return False

    # 3) ë‚ ì§œ ì»¬ëŸ¼ ì‹¤ì œ íŒŒì‹± ë¹„ìœ¨ ì²´í¬
    parsed_date_col = None
    for c in date_cols:
        s = pd.to_datetime(df[c].astype(str).str.strip(), errors="coerce", format=None)
        valid_ratio = s.notna().mean()
        if valid_ratio >= 0.6:     # 60% ì´ìƒë§Œ ë‚ ì§œë¡œ ì½í˜€ë„ ì¸ì •
            parsed_date_col = c
            df["_parsed_dt_"] = s
            break
    if parsed_date_col is None:
        return False

    # 4) == í•µì‹¬ ì¶”ê°€ ==
    #    supply_chain ë°ì´í„°ì²˜ëŸ¼ ê°™ì€ ë‚ ì§œì— í’ˆëª©/ì°½ê³  ë‹¨ìœ„ë¡œ ì—¬ëŸ¬ í–‰ì´ ìˆëŠ” ê²½ìš°:
    #    - í’ˆëª©/ì œí’ˆ/sku/warehouse ê°™ì€ í‚¤ê°€ ìˆìœ¼ë©´ ì´ê²ƒë„ ì‹œê³„ì—´ë¡œ ë³¸ë‹¤.
    key_like_keywords = ["product", "item", "sku", "material", "warehouse", "location", "store", "shop", "code"]
    has_entity_key = any(
        any(k in h.strip().lower() for k in key_like_keywords)
        for h in raw_headers
    )

    # ë‚ ì§œê°€ ìˆê¸´ í•œë°, í•˜ë£¨ì— í–‰ì´ ë„ˆë¬´ ë§ìœ¼ë©´ íŒ¨ë„ë¡œ ì˜ì‹¬
    per_day_counts = None
    if "_parsed_dt_" in df.columns:
        per_day_counts = df.groupby(df["_parsed_dt_"].dt.date).size()
    is_panel_shape = per_day_counts is not None and per_day_counts.mean() > 1.5

    # 5) horizon / forecast íŒ¨í„´ ê°ì§€ (ì´ê²Œ ìˆìœ¼ë©´ ê±°ì˜ 100% ì˜ˆì¸¡ìš©)
    horizon_patterns = [
        r"t\+\d+",
        r"\bforecast\b",
        r"\bì˜ˆìƒ\s*ìˆ˜ì£¼ëŸ‰",
        r"\bì˜ˆì •\s*ìˆ˜ì£¼ëŸ‰",
        r"\bprediction",
    ]
    hdr_join = " || ".join(lower_headers)
    has_horizon = any(re.search(pat, hdr_join, flags=re.IGNORECASE) for pat in horizon_patterns)

    if has_horizon:
        # ì´ê±´ ë„¤ supply_chain_dataê°€ ì—¬ê¸°ì— ê±¸ë¦°ë‹¤
        return True

    # 6) ì¼ë°˜ì ì¸ ì‹œê°„ ìˆœì„œì„±ë„ í•œ ë²ˆì€ ë³¸ë‹¤ (ë‹¨, íŒ¨ë„ì´ë©´ ì´ ì²´í¬ëŠ” ìŠ¤í‚µ)
    if not is_panel_shape:
        dt_sorted = df["_parsed_dt_"].dropna().sort_values()
        diffs = dt_sorted.diff().dropna().dt.total_seconds()
        if len(diffs) == 0:
            return False
        # ê°„ê²© ë„ˆë¬´ ë“¤ì­‰ë‚ ì­‰í•˜ë©´ ì‹œê³„ì—´ë¡œ ì•ˆ ë³¸ë‹¤
        mean_gap = np.mean(diffs)
        std_gap = np.std(diffs)
        if mean_gap > 0 and (std_gap / mean_gap) > 5.0:
            # ë“¤ì­‰ë‚ ì­‰ â†’ ë¡œê·¸ì„± ë°ì´í„°ì¼ ìˆ˜ ìˆìŒ
            return False

    # 7) ìˆ«ìí˜• í”¼ì²˜ ê°œìˆ˜ (ìˆ˜ìš”ì˜ˆì¸¡/ì‹œê³„ì—´ì´ë©´ ë³´í†µ 2ê°œ ì´ìƒ)
    num_like_cnt = sum(1 for t in col_types if any(x in t.lower() for x in ["int", "decimal", "float"]))
    if num_like_cnt < 1 and not has_horizon:
        # ìˆ«ìë„ ì—†ê³  horizonë„ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì¼ë°˜ í…Œì´ë¸”ë¡œ
        return False

    # 8) íŒ¨ë„ + ë‚ ì§œ + ì—”í‹°í‹°í‚¤ ìˆìœ¼ë©´ ì‹œê³„ì—´ë¡œ ë³¸ë‹¤
    if is_panel_shape and has_entity_key:
        return True

    # 9) LLM ë³´ì • (ìˆì„ ë•Œë§Œ)
    if "llm_advise_schema" in globals():
        try:
            ans = llm_advise_schema(raw_headers, col_types, [], ["demand", "sales", "quantity"])
            if isinstance(ans, dict):
                if any("time" in str(v).lower() for v in ans.values()):
                    return True
        except Exception:
            pass

    # ìœ„ ì¡°ê±´ ë‹¤ í†µê³¼í–ˆìœ¼ë©´ True
    return True

def _detect_forecastable_like(headers: list[str], text_data: str, col_types: list[str]) -> bool:
    """
    'ì´ CSVë¡œ ìˆ˜ìš”ì˜ˆì¸¡(íŒë§¤ëŸ‰/ì¶œê³ ëŸ‰/ìˆ˜ì£¼ëŸ‰/ì¶œê³ ëŸ‰ ë“±)ì„ ëŒë¦´ ìˆ˜ ìˆëƒ?' íŒì •ìš©.
    - ë¨¼ì € 'ì¸ì‚¬/ê¸‰ì—¬/ì¡°ì§' ê°™ì€ ë¹„ì˜ˆì¸¡ ë„ë©”ì¸ì€ ë°”ë¡œ ì»·
    - ê·¸ ë‹¤ìŒì— 'ë‚ ì§œ + ìˆ˜ëŸ‰/ë§¤ì¶œ/ì£¼ë¬¸/ì¶œê³  ê³„ì—´ íƒ€ê¹ƒ' ìˆëŠ”ì§€ ë³¸ë‹¤
    - ë§ˆì§€ë§‰ìœ¼ë¡œ íŒ¨ë„/ë‹¨ì¼ì‹œê³„ì—´ í˜•íƒœê°€ ë˜ëŠ”ì§€ ë³¸ë‹¤
    """
    import csv, io, re
    import pandas as pd
    import numpy as np

    raw_headers = headers
    lower_headers = [h.strip().lower() for h in raw_headers]

    # 0. ì™„ì „ ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ í•„í„° (HR/ê¸‰ì—¬/ì¡°ì§ ê´€ë¦¬) â†’ ì´ëŸ° ê±´ ìˆ˜ìš”ì˜ˆì¸¡ ì•„ë‹ˆë¼ê³  ë´ì•¼ í•¨
    hr_like_keywords = [
        "ì‚¬ì›", "ì‚¬ë²ˆ", "ì§ì›", "employee", "emp_", "staff", "ì¸ì‚¬", "hr",
        "dept", "department", "ë¶€ì„œ", "íŒ€ëª…", "team", "position", "ì§ê¸‰",
        "ê¸‰ì—¬", "ê¸‰ì—¬ì•¡", "salary", "pay", "wage", "ì—°ë´‰", "ì›”ê¸‰", "ì‹œê¸‰",
        "ì„±ê³¼", "í‰ê°€", "ì…ì‚¬", "ì…ì‚¬ì¼", "í‡´ì‚¬", "í‡´ì‚¬ì¼",
        "ì£¼ë¯¼", "address", "ì „í™”", "tel", "íœ´ëŒ€í°", "email"
    ]
    # ê¸‰ì—¬/ì§ì› ë‹¨ì–´ê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ”ë°, ì•„ë˜ì˜ ì§„ì§œ ìˆ˜ìš” íƒ€ê¹ƒ ë‹¨ì–´ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë°”ë¡œ False
    demand_like_keywords = [
        "ìˆ˜ìš”", "demand", "íŒë§¤", "sales", "ì¶œê³ ", "shipment", "ship_qty",
        "ë°œì£¼", "order", "ì£¼ë¬¸", "forecast", "ì˜ˆì¸¡", "ìˆ˜ì£¼", "ë‚©í’ˆ",
        "qty", "quantity", "ìˆ˜ëŸ‰", "ë§¤ì¶œ", "revenue", "consumption",
        "ì¬ê³ ", "stock", "inventory"
    ]
    has_hr_word = any(any(k in h for k in hr_like_keywords) for h in lower_headers)
    has_demand_word = any(any(k in h for k in demand_like_keywords) for h in lower_headers)
    if has_hr_word and not has_demand_word:
        # "ì§ì›ê¸‰ì—¬.xlsx" ê°™ì€ ê±´ ì—¬ê¸°ì„œ ë§‰íŒë‹¤
        return False

    # 1. CSV -> DataFrame ìœ¼ë¡œ ì ê¹ ì½ì–´ì„œ ì‹¤ì œ ê°’ ë´„
    csv_buf = io.StringIO(text_data)
    reader = csv.reader(csv_buf)
    rows = list(reader)

    if not rows:
        return False

    header_row = rows[0]
    data_rows = rows[1:]

    # ë°ì´í„°ê°€ ë„ˆë¬´ ì—†ìœ¼ë©´ ì˜ˆì¸¡ ì˜ë¯¸ X
    if len(data_rows) < 3:
        return False

    # pandasë¡œ í•œ ë²ˆ ë”
    df = pd.DataFrame(data_rows, columns=header_row)

    # 2. ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
    date_candidates = []
    date_name_keywords = [
        "date", "day", "ë‚ ì§œ", "ì¼ì", "ê¸°ì¤€ì¼", "base_date",
        "dt", "ym", "yyyymm", "year", "month", "week"
    ]
    for col in df.columns:
        col_l = col.strip().lower()
        if any(k in col_l for k in date_name_keywords):
            date_candidates.append(col)

    # ì´ë¦„ìœ¼ë¡œ ëª» ì°¾ì•˜ìœ¼ë©´ ì‹¤ì œ ê°’ìœ¼ë¡œ ì°¾ê¸°
    if not date_candidates:
        for col in df.columns:
            try:
                parsed = pd.to_datetime(df[col], errors="coerce", infer_datetime_format=True)
            except Exception:
                continue
            if parsed.notna().sum() >= max(3, int(len(df) * 0.3)):
                date_candidates.append(col)

    if not date_candidates:
        # ë‚ ì§œ í•˜ë‚˜ë„ ëª» ì°¾ìœ¼ë©´ ì˜ˆì¸¡ ë°ì´í„°ë¡œ ë³´ê¸° ì–´ë µë‹¤
        return False

    # 3. íƒ€ê¹ƒ ì»¬ëŸ¼(ìˆ˜ëŸ‰/ë§¤ì¶œ/ì£¼ë¬¸/ì¶œê³ /ì¬ê³ ) ì°¾ê¸°
    qty_name_keywords = [
        "qty", "quantity", "ìˆ˜ëŸ‰", "íŒë§¤", "sales", "ì¶œê³ ", "shipment",
        "order_qty", "ì£¼ë¬¸", "ë°œì£¼", "ë°œì£¼ëŸ‰", "order_amount",
        "demand", "forecast", "ì˜ˆì¸¡", "ìˆ˜ìš”",
        "stock", "inventory", "onhand", "ì¬ê³ "
    ]
    num_like_cnt = 0
    has_qty_header = False
    for i, col in enumerate(df.columns):
        col_l = col.strip().lower()

        # ì´ë¦„ì´ ìˆ˜ìš”/ìˆ˜ëŸ‰ ê³„ì—´ì´ë©´ ìš°ì„  í›„ë³´
        if any(k in col_l for k in qty_name_keywords):
            has_qty_header = True
            continue

        # ì´ë¦„ì€ ì• ë§¤í•œë° ìˆ«ìí˜•ìœ¼ë¡œ ë³´ì´ë©´ í›„ë³´
        if i < len(col_types):
            if col_types[i].lower().startswith(("int", "bigint", "decimal", "float", "double")):
                num_like_cnt += 1

    # ì§„ì§œë¡œ target ì»¬ëŸ¼ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì»·
    if not has_qty_header and num_like_cnt == 0:
        return False

    # 4. ì—”í‹°í‹° í‚¤(í’ˆëª©/ìƒí’ˆ/ì½”ë“œ/ì°½ê³ ) ìˆëŠ”ì§€
    entity_keywords = [
        "item", "item_code", "item_cd", "product", "product_code", "prod_cd",
        "sku", "í’ˆëª©", "í’ˆë²ˆ", "í’ˆëª©ì½”ë“œ", "ëª¨ë¸", "model",
        "ê³ ê°", "customer", "ê±°ë˜ì²˜", "account",
        "ì°½ê³ ", "wh", "warehouse", "store", "ì§€ì ", "branch", "ë§¤ì¥"
    ]
    has_entity_key = any(any(k in h for k in entity_keywords) for h in lower_headers)

    # 5. ì‹¤ì œ ë‚ ì§œ íŒŒì‹±í•´ì„œ ê°„ê²© ì¢€ ë³¸ë‹¤
    # ê°€ì¥ ì²« ë²ˆì§¸ ë‚ ì§œ í›„ë³´ë§Œ ë³¸ë‹¤ (ëŒ€ë¶€ë¶„ 1ê°œ)
    dtcol = date_candidates[0]
    parsed_dt = pd.to_datetime(df[dtcol], errors="coerce", infer_datetime_format=True)
    if parsed_dt.notna().sum() < 3:
        return False
    df["_parsed_dt_"] = parsed_dt

    # 6. ë°ì´í„°ê°€ "ì§ì› ë‹¨ìœ„"ë¡œ ë³´ì´ëŠ”ì§€ í•œ ë²ˆ ë” í•„í„°
    # ì˜ˆ: ì‚¬ë²ˆ/ì§ì›ëª…/ë¶€ì„œëª… + ë‚ ì§œ + ìˆ«ì(ê¸‰ì—¬) â†’ ì´ê±´ ìœ„ì—ì„œ í•œ ë²ˆ ê±¸ë €ì§€ë§Œ
    # ê·¸ë˜ë„ date + salary ê°€ ë‚¨ì„ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ í•œ ë²ˆ ë”
    salary_like = ["salary", "ê¸‰ì—¬", "pay", "wage", "ì—°ë´‰", "ì›”ê¸‰"]
    if any(any(k in h for k in salary_like) for h in lower_headers) and not has_qty_header:
        return False

    # 7. íŒ¨ë„ í˜•íƒœì¸ì§€(ê°™ì€ ë‚ ì§œì— ì—¬ëŸ¬ í–‰) í™•ì¸
    # ë‚ ì§œ + ì—”í‹°í‹° ìˆìœ¼ë©´ íŒ¨ë„ë¡œ ë³´ê³  OK
    grp = df.groupby("_parsed_dt_").size()
    is_panel_shape = bool((grp > 1).any())

    # 8. ë‚ ì§œ ê°„ê²© ë„ˆë¬´ ë“¤ì­‰ë‚ ì­‰í•œ ì´ë²¤íŠ¸ ë¡œê·¸ëŠ” ì œì™¸
    dt_sorted = df["_parsed_dt_"].dropna().sort_values()
    diffs = dt_sorted.diff().dropna().dt.total_seconds()
    if len(diffs) >= 2:
        mean_gap = float(np.mean(diffs))
        std_gap = float(np.std(diffs))
        # gapì´ ë„ˆë¬´ ëœë¤ì´ë©´ ë¡œê·¸ì„± â†’ ì˜ˆì¸¡ ì í•©ë„ ë‚®ìŒ
        if mean_gap > 0 and (std_gap / mean_gap) > 8.0:
            return False

    # --- ìµœì¢… íŒë‹¨ ---
    # 1) ë‚ ì§œ ìˆê³ 
    # 2) ìˆ˜ëŸ‰/ë§¤ì¶œ/ì£¼ë¬¸/ì¬ê³  ê³„ì—´ target ìˆê³ 
    # 3) (íŒ¨ë„+ì—”í‹°í‹°)ê±°ë‚˜ ë‹¨ì¼ì‹œê³„ì—´ì´ë©´ OK
    if is_panel_shape:
        if has_entity_key and (has_qty_header or num_like_cnt >= 1):
            return True
        # ì—”í‹°í‹°ê°€ ì—†ëŠ”ë° ê°™ì€ ë‚ ì§œì— ì—¬ëŸ¬ ê±´ì´ë©´ ë¡œê·¸ì— ê°€ê¹Œìš°ë‹ˆ ë³´ìˆ˜ì ìœ¼ë¡œ False
        return False
    else:
        # ë‹¨ì¼ ì‹œê³„ì—´: ë‚ ì§œ í•œ ì¤„ì”© + íƒ€ê¹ƒ í•˜ë‚˜
        if has_qty_header or num_like_cnt == 1:
            return True

    return False




def _find_first_record_bytes(data: bytes) -> bytes:
    in_quote = False
    i = 0
    while i < len(data):
        b = data[i]
        if b == 0x22:  # "
            if in_quote and i + 1 < len(data) and data[i+1] == 0x22:
                i += 2
                continue
            in_quote = not in_quote
            i += 1
            continue
        if (b in (0x0A, 0x0D)) and not in_quote:
            return data[:i]
        i += 1
    return data

def _parse_headers_from_first_line(first_line_text: str) -> List[str]:
    s = first_line_text.replace("\r", "").replace("\n", "")
    reader = csv.reader([s])
    try:
        headers = next(reader, [])
    except Exception:
        headers = []
    return headers

def _assert_csv_filename(filename: str):
    if not filename or "." not in filename or filename.split(".")[-1].lower() != "csv":
        raise HTTPException(status_code=400, detail="CSV íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤(.csv í™•ì¥ì í•„ìš”).")

def _escape_mysql_identifier(name: str) -> str:
    return f"`{name.replace('`','``')}`"

def _sanitize_table_name(filename: str) -> str:
    # í™•ì¥ì ì œê±° â†’ ë¹„í—ˆìš©ë¬¸ì '_' ì¹˜í™˜ â†’ ê¸¸ì´ ì œí•œ â†’ ë¹„ì–´ìˆìœ¼ë©´ ë³´ì •
    name = os.path.splitext(filename)[0]
    name = re.sub(r"[^0-9A-Za-zê°€-í£_]+", "_", name).strip("_")
    if not name:
        name = "table_" + uuid.uuid4().hex[:8]
    if len(name) > 64:
        name = name[:64]
    return name

def _get_existing_table_schema(conn, db_name: str, table_name: str):
    rows = conn.execute(text("""
        SELECT column_name, column_type, is_nullable, ordinal_position
        FROM information_schema.columns
        WHERE table_schema = :db AND table_name = :tbl
        ORDER BY ordinal_position ASC
    """), {"db": db_name, "tbl": table_name}).fetchall()
    return [
        {"name": r[0], "column_type": str(r[1]).lower(), "nullable": (str(r[2]).lower()=="yes")}
        for r in rows
    ]


def _same_schema(existing: List[Dict[str, Any]], headers: List[str]) -> bool:
    if len(existing) != len(headers):
        return False
    for e, h in zip(existing, headers):
        if e["name"] != h:
            return False
        # ìš°ë¦¬ê°€ ë§Œë“  í…Œì´ë¸” ê¸°ì¤€: varchar(255) / nullable
        if ("varchar" not in e["column_type"]) or (e["nullable"] is not True):
            return False
    return True



# [ADD] í—¤ë” ì •ì œê¸°: ê³µë°±/ì œì–´ë¬¸ì ì œê±°, ì™„ì „ ë¹ˆ ê°’ ë³´ì •, ì¶©ëŒ ì‹œ __2, __3 ë¶€ì—¬
def _sanitize_headers_for_mysql(headers: list[str]) -> tuple[list[str], list[tuple[str, str]], list[str]]:
    """
    returns: (cleaned_headers, mapping[(original, cleaned)], warnings)
    """
    cleaned: list[str] = []
    mapping: list[tuple[str, str]] = []
    warnings: list[str] = []

    # 1) 1ì°¨ ì •ì œ: trim, ì œì–´ë¬¸ì ì œê±°, ë‚´ë¶€ ë‹¤ì¤‘ ê³µë°± ì ‘ê¸°
    for i, h in enumerate(headers):
        orig = str(h or "")
        s = orig.strip()
        # ì œì–´ë¬¸ì ì œê±°
        s = re.sub(r"[\x00-\x1f\x7f]", "", s)
        # ë‚´ë¶€ ë‹¤ì¤‘ ê³µë°± -> í•˜ë‚˜ë¡œ
        s = re.sub(r"\s+", " ", s)

        # ì™„ì „ ë¹ˆ ê°’ì´ë©´ ìë™ ì´ë¦„ ë¶€ì—¬
        if s == "":
            s = f"col_{i+1}"
            warnings.append(f"ë¹ˆ ì»¬ëŸ¼ëª… ê°ì§€ â†’ '{orig}' ë¥¼ '{s}' ë¡œ ëŒ€ì²´")

        mapping.append((orig, s))
        cleaned.append(s)

    # 2) ì¤‘ë³µ í•´ê²°: ë™ì¼ ì´ë¦„ ìˆìœ¼ë©´ __2, __3...
    seen: dict[str, int] = {}
    for i, s in enumerate(cleaned):
        base = s
        n = seen.get(base, 0)
        if n == 0:
            seen[base] = 1
            continue
        # ì´ë¯¸ ì¡´ì¬ â†’ ì ‘ë¯¸ì‚¬ ì¦ê°€
        while True:
            n += 1
            cand = f"{base}__{n}"
            if cand not in seen:
                cleaned[i] = cand
                seen[cand] = 1
                warnings.append(f"ì¤‘ë³µ ì»¬ëŸ¼ëª… ì¶©ëŒ â†’ '{base}'ë¥¼ '{cand}' ë¡œ ë³€ê²½")
                break
        seen[base] = n

    return cleaned, mapping, warnings

# ---------------------------
# Schemas (IO)
# ---------------------------
class SchemaCol(BaseModel):
    name: str
    mysql_type: str = Field(default="VARCHAR(255)")
    nullable: bool = Field(default=True)

class CreateTableRequest(BaseModel):
    table_name: str
    schema: List[SchemaCol]

class CreateTableResponse(BaseModel):
    ddl: str
    dry_run: bool = True

class StatsRequest(BaseModel):
    table_name: str

class StatsResponse(BaseModel):
    table_name: str
    row_count: int
    column_count: int

class UploadResponse(BaseModel):
    file_id: str
    filename: str
    headers: List[str]
    header_signature: str
    header_hash: str
    num_columns: int
    table_name: Optional[str] = None
    table_action: Optional[str] = None   # created|merged|replaced|error:...
    staged_rows: Optional[int] = None
    merged_rows: Optional[int] = None
    header_translation: Optional[Dict[str, str]] = None  # ì›ë³¸ í—¤ë” -> ìµœì¢… í—¤ë”
    header_warnings: List[str] = []                      # ì •ì œ ê²½ê³ ë“¤
    is_forecastable: Optional[bool] = None

# ---------------------------
# Health
# ---------------------------
@app.get("/_health")
def health():
    return {"engine": bool(ENGINE), "startup_ok": STARTUP_OK}

# ---------------------------
# UPLOAD â†’ ìë™ ìƒì„±/ë³‘í•©/êµì²´ + í•©ì§‘í•© ì ì¬
# ---------------------------
@app.post("/upload", response_model=UploadResponse)
async def upload_csv(file: UploadFile = File(...)):
    _assert_csv_filename(file.filename)

    raw = await file.read()
    if not raw:
        raise HTTPException(status_code=400, detail="ë¹ˆ íŒŒì¼ì…ë‹ˆë‹¤.")

    # í—¤ë” íŒŒì‹±
    first_line_bytes = _find_first_record_bytes(raw)
    if not first_line_bytes:
        raise HTTPException(status_code=400, detail="í—¤ë” ì¤„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    header_signature_text = _decode_bytes(first_line_bytes).rstrip("\r\n")
    headers = _parse_headers_from_first_line(header_signature_text)
    
    if not headers:
        raise HTTPException(status_code=400, detail="í—¤ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    headers_original = list(headers)
    
    # âœ… í—¤ë” ì •ì œ ì ìš© (trim, ê³µë°± ì ‘ê¸°, ë¹ˆ ì»¬ëŸ¼ ìë™ëª…, ì¤‘ë³µ ì¶©ëŒ í•´ê²°)
    clean_headers, mapping, hdr_warnings = _sanitize_headers_for_mysql(headers)
    headers = clean_headers

    # 1) ì›ë³¸â†’ì •ì œ & ê¸°ë³¸ ì›ë³¸â†’ìµœì¢…
    original_to_clean = {orig: clean for (orig, clean) in mapping}
    original_to_final = original_to_clean.copy()

    # 2) (ì˜µì…˜) í—¤ë” ë²ˆì—­
    want_translate_hdr = os.getenv("TRANSLATE_HEADERS", "false").lower() in ("1","true","yes")
    all_ascii = all(_is_ascii_identifier(h) for h in headers)

    if want_translate_hdr and not all_ascii and _translate_table_name is not None:
        translated = []
        for h in headers:
            try:
                en = _translate_table_name(h)
                translated.append(en or h)
            except Exception as e:
                logging.warning("í—¤ë” ë³€í™˜ ì‹¤íŒ¨(%r) â†’ ì›ë³¸ ì‚¬ìš©: %s", e, h)
                translated.append(h)
        headers = _ensure_unique_names(translated)

        # ì •ì œâ†’ìµœì¢…
        clean_to_final = dict(zip(clean_headers, headers))
        # ì›ë³¸â†’ìµœì¢… = í•©ì„±
        original_to_final = {
            orig: clean_to_final.get(original_to_clean.get(orig, orig), original_to_clean.get(orig, orig))
            for orig in headers_original
        }
    else:
        logging.info("Header translation skipped (want=%s, ascii=%s, has_llm=%s)",
                    want_translate_hdr, all_ascii, _translate_table_name is not None)




    header_hash = hashlib.sha256(first_line_bytes).hexdigest()

    # ì›ë³¸ ì €ì¥
    file_id = str(uuid.uuid4())
    save_path = os.path.join(UPLOAD_DIR, f"{file_id}.csv")
    with open(save_path, "wb") as f:
        f.write(raw)

    meta = UploadMeta(
        file_id=file_id,
        filename=file.filename,
        headers=headers,
        header_signature=header_signature_text,
        header_hash=header_hash,
        num_columns=len(headers),
        uploaded_at=datetime.now(timezone.utc).isoformat()
    )
    UPLOADS[file_id] = meta

    text_data = _decode_bytes(raw)
    col_types = _infer_mysql_types_from_sample(headers, text_data, sample_rows=2000)
    col_types = _shrink_types_to_fit(col_types, limit=65000)

    # --- ìˆ˜ìš”ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°ì´í„° ì—¬ë¶€ ---
    is_fc_clean = _detect_forecastable_like(clean_headers, text_data, col_types)
    is_fc_final = _detect_forecastable_like(headers, text_data, col_types)
    is_forecastable = is_fc_clean or is_fc_final


    original_base = os.path.splitext(file.filename)[0]
    table_name = _sanitize_table_name(file.filename)  # ê¸°ë³¸ í´ë°±

    # âœ… íŒŒì¼ëª…ì´ ì´ë¯¸ ASCIIë©´ ë²ˆì—­ ìŠ¤í‚µ
    original_base = os.path.splitext(file.filename)[0]
    table_name = _sanitize_table_name(file.filename)  # ê¸°ë³¸ í´ë°±

    want_translate_tbl = os.getenv("TRANSLATE_TABLE_NAME", "false").lower() in ("1","true","yes")
    if want_translate_tbl and _translate_table_name is not None and not _is_ascii_identifier(original_base):
        try:
            ai_name = _translate_table_name(original_base)
            if ai_name:
                table_name = ai_name   # translatorê°€ snake_case/ASCII/ê¸¸ì´ ì œí•œ ë³´ì¥
            logging.info("í…Œì´ë¸”ëª… ë²ˆì—­ ì ìš©: %s -> %s", original_base, table_name)
        except Exception as e:
            logging.warning("í…Œì´ë¸”ëª… ë²ˆì—­ ì‹¤íŒ¨(%s) â†’ í´ë°± ì‚¬ìš©: %s", e, table_name)
    else:
        logging.info("í…Œì´ë¸”ëª… ë²ˆì—­ ìŠ¤í‚µ(want=%s, ascii=%s, has_llm=%s)", 
                    want_translate_tbl, _is_ascii_identifier(original_base), _translate_table_name is not None)


    table_action: Optional[str] = None
    staged_rows = 0
    merged_rows = 0

    if ENGINE is not None:
        try:
            with ENGINE.begin() as conn:
                # 1) í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€
                exists = conn.execute(text("""
                    SELECT COUNT(*) FROM information_schema.tables
                    WHERE table_schema=:db AND table_name=:tbl
                """), {"db": DB_NAME, "tbl": table_name}).scalar() or 0

                # 2) (ì¡´ì¬í•˜ë©´) ë“œë¡­ â€“ íƒ€ì…ì´ ë°”ë€” ìˆ˜ ìˆìœ¼ë‹ˆ í•­ìƒ drop-createë¡œ ì¼ê´€í™”
                if exists:
                    conn.execute(text(f"DROP TABLE {_escape_mysql_identifier(table_name)}"))

                # 3) ì¶”ë¡  íƒ€ì…(col_types)ë¡œ ë©”ì¸ í…Œì´ë¸” ìƒì„±
                ddl = _build_ddl_from_headers(table_name, headers, col_types)
                conn.execute(text(ddl))
                table_action = "created" if not exists else "replaced"

                # 4) ìŠ¤í…Œì´ì§• í…Œì´ë¸”ë„ ë™ì¼ íƒ€ì…ìœ¼ë¡œ ìƒì„±
                reader = csv.reader(io.StringIO(text_data))  # text_dataëŠ” í•¨ìˆ˜ ìƒë‹¨ì—ì„œ ì´ë¯¸ ë§Œë“  ê±¸ ì‚¬ìš©
                next(reader, None)  # í—¤ë” ìŠ¤í‚µ
                stg = f"_stg_{uuid.uuid4().hex[:8]}"
                stg_ddl = _build_ddl_from_headers(stg, headers, col_types)
                conn.execute(text(stg_ddl))

                cols_esc = ", ".join(_escape_mysql_identifier(h) for h in headers)
                ph = ", ".join([f":c{i}" for i in range(len(headers))])
                ins_stg = text(f"INSERT INTO {_escape_mysql_identifier(stg)} ({cols_esc}) VALUES ({ph})")

                # 5) ë°°ì¹˜ ì ì¬
                batch: List[Dict[str, Any]] = []
                for row in reader:
                    row = (row + [""] * len(headers))[:len(headers)]
                    vals = {f"c{i}": (v if v != "" else None) for i, v in enumerate(row)}
                    batch.append(vals)
                    if len(batch) >= 5000:
                        conn.execute(ins_stg, batch)
                        staged_rows += len(batch)
                        batch.clear()
                if batch:
                    conn.execute(ins_stg, batch)
                    staged_rows += len(batch)
                    batch.clear()

                # 6) í•©ì§‘í•© ë¨¸ì§€
                on_clause = " AND ".join(
                    [f"t.{_escape_mysql_identifier(h)} <=> s.{_escape_mysql_identifier(h)}" for h in headers]
                )
                merge_sql = text(f"""
                    INSERT INTO {_escape_mysql_identifier(table_name)} ({cols_esc})
                    SELECT {cols_esc}
                    FROM {_escape_mysql_identifier(stg)} s
                    WHERE NOT EXISTS (
                    SELECT 1 FROM {_escape_mysql_identifier(table_name)} t
                    WHERE {on_clause}
                    )
                """)
                res = conn.execute(merge_sql)
                merged_rows = getattr(res, "rowcount", None) or 0

                # 7) ìŠ¤í…Œì´ì§• í…Œì´ë¸” ì œê±°
                conn.execute(text(f"DROP TABLE {_escape_mysql_identifier(stg)}"))

                # ì‹œê³„ì—´ ë©”íƒ€ì •ë³´ ì €ì¥
                conn.execute(
                    text("""
                        INSERT INTO uploads_meta (table_name, is_forecastable)
                        VALUES (:tname, :is_fc)
                        ON DUPLICATE KEY UPDATE is_forecastable = VALUES(is_forecastable)
                    """),
                    {"tname": table_name, "is_fc": 1 if is_forecastable else 0},
                )

        except Exception as e:
            table_action = f"error: {_humanize_sql_error(e)}"


    return UploadResponse(
        file_id=file_id,
        filename=file.filename,
        headers=headers,
        header_signature=header_signature_text,
        header_hash=header_hash,
        num_columns=len(headers),
        table_name=table_name,
        table_action=table_action,
        staged_rows=staged_rows,
        merged_rows=merged_rows,
        header_translation=original_to_final,
        header_warnings=hdr_warnings,
        is_forecastable=is_forecastable,
    )

# ---------------------------
# (ì„ íƒ) DDL í”„ë¦¬ë·°/ì‹¤í–‰
# ---------------------------
class InferSchemaResponse(BaseModel):
    schema: List[SchemaCol]
    notes: List[str] = Field(default_factory=lambda: ["mock: echo-only"])

class InferSchemaRequest(BaseModel):
    file_id: str
    headers: List[str]

@app.post("/infer_schema", response_model=InferSchemaResponse)
def infer_schema(req: InferSchemaRequest):
    up = UPLOADS.get(req.file_id)
    if not up:
        raise HTTPException(status_code=400, detail="unknown file_id")
    if req.headers != up.headers:
        raise HTTPException(status_code=400, detail="headers mismatch: /upload ì‘ë‹µ ê·¸ëŒ€ë¡œ ë³´ë‚´ì„¸ìš”.")
    return InferSchemaResponse(schema=[SchemaCol(name=h) for h in req.headers])

@app.post("/create_table", response_model=CreateTableResponse)
def create_table(req: CreateTableRequest, exec: bool = Query(False, alias="exec")):
    if not req.table_name or not req.table_name.strip():
        raise HTTPException(status_code=400, detail="table_nameì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    if not req.schema:
        raise HTTPException(status_code=400, detail="schemaê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    t = _escape_mysql_identifier(req.table_name.strip())
    cols = []
    for c in req.schema:
        cols.append(f"  {_escape_mysql_identifier(c.name)} {c.mysql_type} " + ("NULL" if c.nullable else "NOT NULL"))
    ddl = f"CREATE TABLE {t} (\n" + ",\n".join(cols) + "\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;"
    DDL_LOG[req.table_name] = ddl
    if not exec:
        return CreateTableResponse(ddl=ddl, dry_run=True)
    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB ì—”ì§„ ë¯¸ì´ˆê¸°í™”")
    try:
        with ENGINE.begin() as conn:
            conn.execute(text(ddl))
        return CreateTableResponse(ddl=ddl, dry_run=False)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"DDL ì‹¤í–‰ ì‹¤íŒ¨: {e}")

# ---------------------------
# Stats
# ---------------------------
@app.post("/stats", response_model=StatsResponse)
def stats(req: StatsRequest):
    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB ì—”ì§„ ë¯¸ì´ˆê¸°í™”")
    tname = req.table_name.strip()
    if not tname:
        raise HTTPException(status_code=400, detail="table_name required")
    t_esc = _escape_mysql_identifier(tname)
    try:
        with ENGINE.connect() as conn:
            row_count = conn.execute(text(f"SELECT COUNT(*) FROM {t_esc}")).scalar() or 0
            column_count = conn.execute(text("""
                SELECT COUNT(*) FROM information_schema.columns
                WHERE table_schema = :db AND table_name = :tbl
            """), {"db": DB_NAME, "tbl": tname}).scalar() or 0
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"stats failed: {e}")
    return StatsResponse(table_name=tname, row_count=row_count, column_count=column_count)

# ---------------------------
# Debug
# ---------------------------
@app.get("/_debug/uploads")
def list_uploads() -> Dict[str, Any]:
    return {fid: UPLOADS[fid].model_dump() for fid in UPLOADS}

@app.get("/_debug/ddl/{key}")
def get_logged_ddl(key: str) -> Dict[str, Any]:
    return {"key": key, "ddl": DDL_LOG.get(key)}

# --- ADD: DB ë‚´ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ---
@app.get("/tables")
def list_tables():
    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB ì—”ì§„ ë¯¸ì´ˆê¸°í™”: .env/DB ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    try:
        with ENGINE.connect() as conn:
            rows = conn.execute(text("""
                SELECT table_name
                FROM information_schema.tables
                WHERE table_schema = :db
                ORDER BY table_name
            """), {"db": DB_NAME}).fetchall()
        return {"tables": [r[0] for r in rows]}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"list tables failed: {e}")
    
@app.get("/timeseries-tables")
def get_timeseries_tables():
    """
    uploads_metaì—ì„œ is_timeseries=1 ì¸ í…Œì´ë¸”ëª…ë§Œ ëŒë ¤ì¤Œ
    """
    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB engine not ready")
    with ENGINE.begin() as conn:
        rows = conn.execute(text("""
            SELECT table_name
            FROM uploads_meta
            WHERE is_forecastable = 1
            ORDER BY id DESC
        """)).fetchall()
    return {"tables": [r[0] for r in rows]}

@app.get("/all-tables")
def list_all_tables():
    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB engine not ready")
    with ENGINE.connect() as conn:
        rows = conn.execute(text("""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = :db
              AND table_type = 'BASE TABLE'
            ORDER BY table_name
        """), {"db": DB_NAME}).fetchall()
    return {"tables": [r[0] for r in rows]}


class ColumnsRequest(BaseModel):
    table_name: str

@app.post("/table-columns")
def get_table_columns(req: ColumnsRequest):
    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB engine not ready")

    tbl = req.table_name.strip()
    if not tbl:
        raise HTTPException(status_code=400, detail="table_name is required")

    with ENGINE.begin() as conn:
        rows = conn.execute(text("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = :db AND table_name = :tbl
            ORDER BY ordinal_position
        """), {"db": DB_NAME, "tbl": tbl}).fetchall()

    return {"table_name": tbl, "columns": [r[0] for r in rows]}

@app.get("/table/{table_name}/preview")
def preview_table(table_name: str, limit: int = 100):
    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB engine not initialized")
    t_esc = _escape_mysql_identifier(table_name.strip())
    try:
        with ENGINE.connect() as conn:
            cols = [r[0] for r in conn.execute(text(
                "SELECT column_name FROM information_schema.columns "
                "WHERE table_schema = :db AND table_name = :tbl "
                "ORDER BY ordinal_position"
            ), {"db": DB_NAME, "tbl": table_name}).fetchall()]
            rows = conn.execute(
                text(f"SELECT * FROM {t_esc} LIMIT :limit"), {"limit": limit}
            ).fetchall()
        return {"columns": cols, "rows": [list(r) for r in rows]}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/table/{table_name}/download")
def download_table_csv(table_name: str):
    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB engine not initialized")
    t_esc = _escape_mysql_identifier(table_name.strip())
    try:
        with ENGINE.connect() as conn:
            cols = [r[0] for r in conn.execute(text(
                "SELECT column_name FROM information_schema.columns "
                "WHERE table_schema = :db AND table_name = :tbl "
                "ORDER BY ordinal_position"
            ), {"db": DB_NAME, "tbl": table_name}).fetchall()]
            rows = conn.execute(text(f"SELECT * FROM {t_esc}")).fetchall()
        buf = io.StringIO()
        writer = csv.writer(buf)
        writer.writerow(cols)
        for r in rows: writer.writerow(r)
        buf.seek(0)
        return StreamingResponse(
            iter([buf.getvalue()]),
            media_type="text/csv",
            headers={"Content-Disposition": f'attachment; filename="{table_name}.csv"'}
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

# === main.py ===
from pydantic import BaseModel
from typing import List, Any

class ReplaceRequest(BaseModel):
    table_name: str
    columns: List[str]
    rows: List[List[Any]]

class SaveAsRequest(BaseModel):
    src_table: str
    new_table: str
    columns: List[str]
    rows: List[List[Any]]

def _esc(name: str) -> str:
    # MySQL ì˜ˆì•½ì–´/ê¸°í˜¸ ëŒ€ì‘: ë°˜ë“œì‹œ ì‹ë³„ì ì´ìŠ¤ì¼€ì´í”„
    if not name: raise ValueError("empty identifier")
    if "`" in name: raise ValueError("backtick in identifier")
    return f"`{name}`"

@app.post("/table/replace")
def replace_table(req: ReplaceRequest):
    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB engine not initialized")
    t = _esc(req.table_name.strip())
    if not req.columns:
        raise HTTPException(status_code=400, detail="columns required")

    cols = [ _esc(c) for c in req.columns ]
    placeholders = ", ".join([f":c{i}" for i in range(len(cols))])
    col_list = ", ".join(cols)

    # ëŒ€ìš©ëŸ‰ ì•ˆì „: ì²­í¬ ë‹¨ìœ„ insert
    CHUNK = 1000

    try:
        with ENGINE.begin() as conn:
            conn.execute(text(f"TRUNCATE TABLE {t}"))
            ins = text(f"INSERT INTO {t} ({col_list}) VALUES ({placeholders})")
            n = 0
            for i in range(0, len(req.rows), CHUNK):
                chunk = req.rows[i:i+CHUNK]
                params = []
                for r in chunk:
                    p = { f"c{j}": (r[j] if j < len(req.columns) else None) for j in range(len(req.columns)) }
                    params.append(p)
                if params:
                    conn.execute(ins, params)
                    n += len(params)
        return {"status": "ok", "inserted": n}
    except Exception as ex:
        short = _humanize_sql_error(ex)
        raise HTTPException(status_code=400, detail=f"replace failed: {short}")

@app.post("/table/save_as")
def save_as(req: SaveAsRequest):
    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB engine not initialized")
    src = _esc(req.src_table.strip())
    dst = _esc(req.new_table.strip())
    if not req.columns:
        raise HTTPException(status_code=400, detail="columns required")

    cols = [ _esc(c) for c in req.columns ]
    placeholders = ", ".join([f":c{i}" for i in range(len(cols))])
    col_list = ", ".join(cols)
    CHUNK = 1000

    try:
        with ENGINE.begin() as conn:
            # ìŠ¤í‚¤ë§ˆ ë³µì œ
            exists = conn.execute(text("""
            SELECT COUNT(*) FROM information_schema.tables
            WHERE table_schema = :db AND table_name = :tbl
            """), {"db": DB_NAME, "tbl": req.new_table}).scalar() or 0
            if exists:
                raise HTTPException(status_code=400, detail=f"save_as failed: í…Œì´ë¸” '{req.new_table}' ì´(ê°€) ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            conn.execute(text(f"CREATE TABLE {dst} LIKE {src}"))

            # ë°ì´í„° ì‚½ì…
            ins = text(f"INSERT INTO {dst} ({col_list}) VALUES ({placeholders})")
            n = 0
            for i in range(0, len(req.rows), CHUNK):
                chunk = req.rows[i:i+CHUNK]
                params = []
                for r in chunk:
                    p = { f"c{j}": (r[j] if j < len(req.columns) else None) for j in range(len(req.columns)) }
                    params.append(p)
                if params:
                    conn.execute(ins, params)
                    n += len(params)
        return {"status": "ok", "table": req.new_table, "inserted": n}
    except Exception as ex:
        short = _humanize_sql_error(ex)
        raise HTTPException(status_code=400, detail=f"save_as failed: {short}")

class AutoTrainRequest(BaseModel):
    headers: list[str]
    rows: list[list]
    targets: list[str]
    horizon: int = 14
    use_llm: bool = True
    table_name: str | None = None

class AutoTrainResult(BaseModel):
    summary: str
    by_target: dict

def _is_datetime_series(series: pd.Series) -> bool:
     # ğŸ”§ 2) ê°™ì€ ì´ë¦„ì˜ ì»¬ëŸ¼ì´ ì—¬ëŸ¬ ê°œë¼ì„œ DataFrame ì´ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° ë°©ì§€
    import pandas as pd
    if isinstance(series, pd.DataFrame):
        series = series.iloc[:, 0]
    try:
        pd.to_datetime(series, errors="raise")
        return True
    except Exception:
        return False

def _choose_time_col(df: pd.DataFrame):
    def _as_series(x):
        return x.iloc[:,0] if isinstance(x, pd.DataFrame) else x

    # í”í•œ ì´ë¦„ ìš°ì„ 
    for c in df.columns:
        lc = str(c).lower()
        if any(k in lc for k in ("date","dt","timestamp","time","ë‚ ì§œ","ì¼ì")):
            if _is_datetime_series(_as_series(df[c])): 
                return c
    # ì „ì²´ ìŠ¤ìº”
    for c in df.columns:
        if _is_datetime_series(_as_series(df[c])): 
            return c
    return None

def _build_tabular_regressor(df: pd.DataFrame, ycol: str):
    X = df.drop(columns=[ycol])
    y = df[ycol].astype(float)

    num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    cat_cols = [c for c in X.columns if c not in num_cols]

    pre = ColumnTransformer([
        ("num", SimpleImputer(strategy="median"), num_cols),
        ("cat", Pipeline([("imp", SimpleImputer(strategy="most_frequent")),
                          ("oh", OneHotEncoder(handle_unknown="ignore"))]), cat_cols)
    ])
    model = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)
    pipe = Pipeline([("pre", pre), ("rf", model)])
    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)
    pipe.fit(Xtr, ytr)
    pred = pipe.predict(Xte)
    mae = float(mean_absolute_error(yte, pred))
    return pipe, {"mae": mae, "n_train": int(len(Xtr)), "n_test": int(len(Xte))}

def _build_ts_with_lags(df: pd.DataFrame, time_col: str, ycol: str, horizon: int):
    # ğŸ”’ ë¨¼ì € safety pick (í˜¹ì‹œ ìƒìœ„ì—ì„œ ë„˜ì–´ì˜¨ ë‘ ì´ë¦„ì´ ë˜ ë³€í–ˆì„ ë•Œ)
    tcol = _pick_existing_col(df, time_col) or time_col
    ycol2 = _pick_existing_col(df, ycol) or ycol
    if ycol2 not in df.columns:
        raise KeyError(f"target column not found: {ycol} / candidates={ [c for c in df.columns if str(c).startswith(f'{ycol}')] }")

    # í•„ìš”í•œ ë‘ ì»¬ëŸ¼ë§Œ ë³µì‚¬
    cols = [tcol, ycol2] if tcol != ycol2 else [tcol]
    use = df.loc[:, cols].copy()

    # ì‹œê°„ ì»¬ëŸ¼ â†’ Series ê°•ì œ + ë³€í™˜
    s_time = use[tcol]
    if isinstance(s_time, pd.DataFrame):
        s_time = s_time.iloc[:, 0]
    use["__ts__"] = pd.to_datetime(s_time, errors="coerce")
    if tcol in use.columns:
        use = use.drop(columns=[tcol])
    tcol = "__ts__"

    # íƒ€ê¹ƒ â†’ Series ê°•ì œ + ìˆ«ìí™”
    s_y = use[ycol2] if ycol2 in use.columns else df[ycol2]
    if isinstance(s_y, pd.DataFrame):
        s_y = s_y.iloc[:, 0]
    s_y = pd.to_numeric(s_y, errors="coerce")
    use[ycol2] = s_y

    use = use.dropna(subset=[tcol]).sort_values(tcol)

    # ë™ í”¼ì²˜
    for L in (1, 2, 3, 7, 14):
        use[f"lag_{L}"] = use[ycol2].shift(L)

    use = use.dropna()
    X = use.drop(columns=[ycol2, tcol])
    y = use[ycol2].astype(float)

    tscv = TimeSeriesSplit(n_splits=5)
    last_mae = None
    model = RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1)
    for tr_idx, te_idx in tscv.split(X):
        Xtr, Xte = X.iloc[tr_idx], X.iloc[te_idx]
        ytr, yte = y.iloc[tr_idx], y.iloc[te_idx]
        model.fit(Xtr, ytr)
        pred = model.predict(Xte)
        last_mae = float(mean_absolute_error(yte, pred))

    # ë¡¤ë§ ì˜ˆì¸¡
    hist = use.tail(max(14, horizon)).copy()
    future = []
    cur = hist.iloc[-1].copy()
    for h in range(horizon):
        row = {k: cur.get(k, np.nan) for k in X.columns}
        yhat = float(model.predict(pd.DataFrame([row]))[0])
        for L in (14, 7, 3, 2, 1):
            key = f"lag_{L}"
            if key in cur.index:
                cur[key] = cur.get(f"lag_{L-1}", yhat if L == 1 else cur[key])
        future.append(yhat)

    return model, {"mae_cv": last_mae, "horizon": horizon, "forecast": future}

@app.post("/auto_train", response_model=AutoTrainResult)
def auto_train(req: AutoTrainRequest):
    # df ìƒì„± ì§í›„ (ì´ë¯¸ ìˆë˜ ì½”ë“œì— ì¶”ê°€/ë³´ê°•)
    df = pd.DataFrame(req.rows, columns=req.headers).copy()

    # (A) ì¤‘ë³µ í—¤ë” ìœ ë‹ˆí¬í™”
    if df.columns.duplicated().any():
        seen = {}
        new_cols = []
        for c in df.columns:
            k = seen.get(c, 0)
            new_cols.append(c if k == 0 else f"{c}__{k+1}")
            seen[c] = k + 1
        df.columns = new_cols

    # (B) ìˆ«ì ë³€í™˜ ì‹œë„ (ë‚ ì§œì²˜ëŸ¼ ìƒê¸´ ê±´ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    for c in df.columns:
        try:
            df[c] = pd.to_numeric(df[c])
        except Exception:
            pass

    # (C) ì‹œê°„ ì»¬ëŸ¼ ì„ íƒ ì‹œì—ë„ ì•ˆì „ í”½ì»¤ ì‚¬ìš©
    raw_time_col = _choose_time_col(df)   # ê¸°ì¡´ í•¨ìˆ˜ í˜¸ì¶œ
    time_col = _pick_existing_col(df, raw_time_col) if raw_time_col else None

    # 2) (ì„ íƒ) LLMì— ìŠ¤í‚¤ë§ˆ/ìƒ˜í”Œ ì „ë‹¬í•´ ìœ í˜• ì¡°ì–¸ ë°›ê¸°
    llm_suggest = None
    if req.use_llm:
        samples = df.head(5).to_dict(orient="records")
        dtypes = {c:str(df[c].dtype) for c in df.columns}
        llm_suggest = llm_advise_schema(list(df.columns), dtypes, samples, req.targets)

    # 3) ì‹œê³„ì—´ time_col í›„ë³´
    time_col = _choose_time_col(df)

    results = {}
    for y in req.targets:
        # ğŸ”’ íƒ€ê¹ƒëª… ë³´ì • (ê°€ì¥ ë¨¼ì €!)
        y_resolved = _pick_existing_col(df, y)
        if not y_resolved:
            results[y] = {"error": f"target '{y}' not found Â· available={list(df.columns)[:50]} ..."}
            continue

        # ìˆ«ìí˜• íŒë‹¨ì€ ë³´ì •ëœ ì´ë¦„ ê¸°ì¤€
        y_is_numeric = (
            pd.api.types.is_numeric_dtype(df[y_resolved]) or
            pd.api.types.is_float_dtype(pd.to_numeric(df[y_resolved], errors="coerce"))
        )

        # LLM & íœ´ë¦¬ìŠ¤í‹± ê²°í•©
        want_ts = False
        if req.use_llm and llm_suggest and isinstance(llm_suggest, dict):
            v = str(llm_suggest.get(y, "")).lower()  # LLMì€ ì‚¬ìš©ìê°€ í´ë¦­í•œ ì›ë˜ ì´ë¦„ìœ¼ë¡œ ë‹µí–ˆì„ ìˆ˜ ìˆìŒ
            want_ts = ("time" in v)

        if not want_ts and time_col and y_is_numeric:
            want_ts = True

        if want_ts and time_col:
            model, meta = _build_ts_with_lags(df, time_col, y_resolved, req.horizon)
            results[y] = {"mode": "time_series_lag_rf", **meta, "resolved": y_resolved}
        else:
            model, meta = _build_tabular_regressor(df, y_resolved)
            snap = model.predict(df.drop(columns=[y_resolved], errors="ignore").head(10))
            results[y] = {"mode": "tabular_rf", **meta, "sample_pred": [float(x) for x in snap], "resolved": y_resolved}

    # 4) ì‘ë‹µ
    summary = f"targets={req.targets} Â· time_col={time_col} Â· llm_used={bool(llm_suggest)}"
    return AutoTrainResult(summary=summary, by_target=results)

from pydantic import BaseModel

class TrainFromTableReq(BaseModel):
    table_name: str
    target_cols: list[str]
    horizon: int = 14

class ForecastTrainRequest(BaseModel):
    table_name: str
    target_cols: list[str]
    time_col: str
    product_col: str
    horizon: int = 14

from fastapi import HTTPException
from pydantic import BaseModel
from sqlalchemy import text
import pandas as pd
import numpy as np
from datetime import timedelta

# ì´ë¯¸ ìœ„ì— ENGINE, DB_NAME ê°™ì€ ê±° ìˆë‹¤ê³  ê°€ì •
# time ì»¬ëŸ¼ ì¶”ì¸¡ìš©
TIME_CANDIDATES = ["date", "dt", "day", "biz_date", "order_date", "forecast_date"]

class TrainFromTableReq(BaseModel):
    table_name: str
    target_cols: list[str] = []
    horizon: int = 14


@app.post("/train_from_table")
def train_from_table(req: ForecastTrainRequest):
    if ENGINE is None:
      raise HTTPException(status_code=500, detail="DB engine not ready")

    tbl = req.table_name.strip()
    if not tbl:
        raise HTTPException(status_code=400, detail="table_name required")

    tgt_cols = req.target_cols or []
    if not tgt_cols:
        raise HTTPException(status_code=400, detail="target_cols required")

    main_target = tgt_cols[0]   # ì—¬ëŸ¬ê°œ ë°›ì•˜ì–´ë„ 1ê°œë§Œ ê·¸ë¦´ ê±°ë¼ ì²«ë²ˆì§¸
    time_col = req.time_col.strip()
    product_col = req.product_col.strip()
    horizon = req.horizon or 14

    # 1) í…Œì´ë¸”ì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ê¸°
    col_list = {time_col, product_col, main_target}
    cols_sql = ", ".join(f"`{c}`" for c in col_list)
    sql = f"SELECT {cols_sql} FROM `{tbl}`"
    df = pd.read_sql(sql, con=ENGINE)

    if df.empty:
        return {"forecast": None, "products": []}

    # 2) ì»¬ëŸ¼ ì •ë¦¬
    df[time_col] = pd.to_datetime(df[time_col], errors="coerce")
    df = df.dropna(subset=[time_col])
    df = df.sort_values(time_col)

    # 3) ì œí’ˆì½”ë“œ ëª©ë¡
    products = df[product_col].dropna().unique().tolist()

    out_series = {}
    for code in products:
        sub = df[df[product_col] == code].copy()
        if sub.empty:
            continue

        history = []
        for _, r in sub.iterrows():
            history.append({
                "date": r[time_col].strftime("%Y-%m-%d"),
                "value": float(r[main_target]) if pd.notna(r[main_target]) else None
            })

        last_date = sub[time_col].max()

        # ====== âœ¨ ì˜ˆì¸¡ ë¡œì§ ê°œì„  ì‹œì‘ ======
        # 1) ìµœê·¼ Nê°œë¡œ ì´ë™í‰ê· 
        WINDOW = 7  # ìµœê·¼ 7ê°œ í‰ê· 
        recent = sub[main_target].dropna().tail(WINDOW)
        if not recent.empty:
            base_val = float(recent.mean())   # ìµœê·¼ í‰ê· 
        else:
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë§ˆì§€ë§‰ ê°’
            base_val = float(sub[main_target].iloc[-1]) if pd.notna(sub[main_target].iloc[-1]) else 0.0

        # 2) ìµœê·¼ ê°’ë“¤ë¡œ ì•„ì£¼ ì•½í•œ ì¶”ì„¸(slope) ê³„ì‚°
        #    (ë§ˆì§€ë§‰ê°’ - ì²«ê°’) / (ê°¯ìˆ˜-1) ìœ¼ë¡œ í•˜ë£¨ ì¦ê°€ëŸ‰ ë¹„ìŠ·í•˜ê²Œ ì¶”ì •
        slope = 0.0
        if len(recent) >= 2:
            start_v = float(recent.iloc[0])
            end_v = float(recent.iloc[-1])
            step = len(recent) - 1
            if step > 0:
                slope = (end_v - start_v) / step   # í•˜ë£¨ë‹¹ ë³€í™”ëŸ‰
        # ë„ˆë¬´ ìš”ë™ì¹˜ë©´ slopeê°€ ë„ˆë¬´ ì»¤ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ clamp
        MAX_SLOPE = base_val * 0.2 if base_val > 0 else 5  # baseì˜ 20%/day ë˜ëŠ” ìµœëŒ€ 5
        if slope > MAX_SLOPE: slope = MAX_SLOPE
        if slope < -MAX_SLOPE: slope = -MAX_SLOPE

        future = []
        for i in range(1, horizon + 1):
            fut_date = (last_date + pd.Timedelta(days=i)).strftime("%Y-%m-%d")
            # 3) ì˜ˆì¸¡ê°’ = ì´ë™í‰ê·  + (ê¸°ìš¸ê¸° * i)
            pred_val = base_val + slope * i
            # ìŒìˆ˜ ì•ˆ ë‚´ë ¤ê°€ê²Œ
            if pred_val < 0:
                pred_val = 0
            future.append({
                "date": fut_date,
                "value": float(round(pred_val, 2))
            })
        # ====== âœ¨ ì˜ˆì¸¡ ë¡œì§ ê°œì„  ë ======


        out_series[str(code)] = {
            "product_code": str(code),
            "history": history,
            "future": future,
        }

    return {
        "table": tbl,
        "time_col": time_col,
        "product_col": product_col,
        "target": main_target,
        "horizon": horizon,
        "products": list(out_series.keys()),
        "series": out_series,
    }

# =====================================================
# âœ… /table-preview : ì§€ì •ëœ í…Œì´ë¸”ì˜ ì¼ë¶€ í–‰ ë¯¸ë¦¬ë³´ê¸°
# =====================================================
from fastapi import Body

@app.post("/table-preview")
def table_preview(
    body: dict = Body(...),
):
    """
    ìš”ì²­ JSON:
      { "table_name": "í…Œì´ë¸”ëª…", "max_rows": 1000 }

    ì‘ë‹µ JSON:
      { "headers": [...], "rows": [ {col:value,...}, ... ] }
    """
    table_name = body.get("table_name")
    max_rows = int(body.get("max_rows", 1000))

    if not table_name:
        raise HTTPException(status_code=400, detail="table_name is required")

    if ENGINE is None:
        raise HTTPException(status_code=500, detail="DB engine not initialized")

    try:
        with ENGINE.begin() as conn:
            # ì»¬ëŸ¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            cols_query = text(f"SHOW COLUMNS FROM `{table_name}`")
            cols = [r[0] for r in conn.execute(cols_query)]
            if not cols:
                raise HTTPException(status_code=404, detail="No columns found")

            # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            preview_query = text(f"SELECT * FROM `{table_name}` LIMIT {max_rows}")
            rows = [dict(r._mapping) for r in conn.execute(preview_query)]

        return {
            "ok": True,
            "table_name": table_name,
            "headers": cols,
            "rows": rows,
            "row_count": len(rows),
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"DB query failed: {e}")
